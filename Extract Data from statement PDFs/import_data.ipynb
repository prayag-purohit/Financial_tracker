{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies: camelot-py[base], opencv-python, ghostscript, pyPDF~=2.0, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import camelot \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory = \"E:\\\\Data analytics projects\\\\Financial Tracker\\\\Statements\"\n",
    "PDF_list = [os.path.join(Directory, x) for x in os.listdir(Directory)]\n",
    "print(PDF_list)\n",
    "print(len(PDF_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##table_areas=['50, 770,500,0 ']\n",
    "#camelot.plot(tables[2], kind='text').show()\n",
    "\n",
    "file_path = str(PDF_list[5])\n",
    "tables = camelot.read_pdf(file_path, pages='all', flavor='stream')\n",
    "all_reports = []\n",
    "\n",
    "# Print accuracy report and save a txt file with aggregated report contents.\n",
    "for x in range(len(tables)):\n",
    "    table = tables[x]\n",
    "    report = table.parsing_report\n",
    "    all_reports.append(f\"Table {x} Report:\\n{report}\\n{'='*40}\\n\")\n",
    "\n",
    "filename = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "report_file_path = f\"E:/Data analytics projects/Financial Tracker/Accuracy Reports/{filename}_aggregate.txt\"\n",
    "\n",
    "# Create a single text file with information about all tables\n",
    "with open(report_file_path, \"w\") as report_file:\n",
    "    report_file.write(f\"{file_path} Accuracy Report\\nNumber of tables: {len(tables)}\\n\\n\")\n",
    "    report_file.write(\"\".join(all_reports))\n",
    "\n",
    "print(f\"Aggregate accuracy report saved to: {report_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = tables[1].df\n",
    "\n",
    "# Create a DataFrame from top_df\n",
    "df = pd.DataFrame(data=top_df)\n",
    "\n",
    "# Find the index of the row that contains the header information\n",
    "header_index = df[df.iloc[:, :2].eq('Date').any(axis=1)].index[0]\n",
    "\n",
    "# Promote the row as the header\n",
    "df.columns = df.iloc[header_index]\n",
    "\n",
    "# Remove all rows before the header row\n",
    "df = df.iloc[header_index + 1:, :]\n",
    "\n",
    "# Remove all columns before the 'Date' column and all columns after the 5th column\n",
    "df = df.loc[:, 'Date':].iloc[:, :5]\n",
    "\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df now contains the cleaned and restructured DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "for x in range(2, len(tables)):\n",
    "    loop_df = tables[x].df\n",
    "    loop_df = pd.DataFrame(data=loop_df)\n",
    "\n",
    "    # Check if the DataFrame has at least 3 rows and 5 columns\n",
    "    if loop_df.shape[0] > 1 and loop_df.shape[1] > 4:\n",
    "        try:\n",
    "            # Find the index of the row that contains the header information\n",
    "            header_index = loop_df[loop_df.iloc[:, :2].eq('Date').any(axis=1)].index[0]\n",
    "            \n",
    "            # Promote the row as the header\n",
    "            loop_df.columns = loop_df.iloc[header_index]\n",
    "            \n",
    "            # Remove all rows before the header row\n",
    "            loop_df = loop_df.iloc[header_index + 1:, :]\n",
    "            \n",
    "            # Remove all columns before the 'Date' column\n",
    "            loop_df = loop_df.loc[:, 'Date':]\n",
    "            \n",
    "            # Save the cleaned DataFrame to the dictionary\n",
    "            dataframes[f\"df{x}\"] = loop_df\n",
    "\n",
    "        except IndexError:\n",
    "            print(f\"Error: Header row not found in df{x}. Skipping...\")\n",
    "            print(loop_df.head())  # Print the head of loop_df\n",
    "            continue\n",
    "\n",
    "\n",
    "print(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataframes) > 0:\n",
    "    # Concatenate all DataFrames in the dictionary\n",
    "    bottom_df = pd.concat(dataframes.values())\n",
    "\n",
    "    # Reset the index\n",
    "    bottom_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Concatenate df and bottom_df to create main_df\n",
    "    main_df = pd.concat([df, bottom_df])\n",
    "    \n",
    "    # Print the expected shape before concatenation\n",
    "    print(f'Expected shape = {df.shape + bottom_df.shape}')\n",
    "    \n",
    "    # Print the shape of the concatenated main_df\n",
    "    print(f'main_df shape: {main_df.shape}')\n",
    "    \n",
    "    # Display the last few rows of main_df\n",
    "    main_df.tail()\n",
    "    \n",
    "else:\n",
    "    main_df = df \n",
    "    print(\"There's only one table in the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.replace('', np.nan, inplace=True)\n",
    "main_df = main_df.dropna(how='all')  # Delete empty rows\n",
    "main_df['is_empty'] = main_df[['Date', 'withdrawn ($)', 'deposited ($)', 'Balance ($)']].isnull().all(axis=1)\n",
    "main_df['is_empty'].value_counts()\n",
    "main_df = main_df.reset_index(drop=True)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(main_df):\n",
    "    if main_df['is_empty'].iloc[i] == True:\n",
    "        # Concatenate Transactions with the previous row\n",
    "        main_df['Transactions'].iloc[i-1] += \" \" + main_df['Transactions'].iloc[i]\n",
    "        \n",
    "        # Drop the current row after concatenating Transactions\n",
    "        main_df = main_df.drop(main_df.index[i])\n",
    "        main_df = main_df.reset_index(drop=True)\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Drop the 'is_empty' column as it's no longer needed\n",
    "main_df = main_df.drop(columns=['is_empty'])\n",
    "\n",
    "# Display the first few rows of the modified DataFrame\n",
    "main_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes, there's a remaining date row with just year value. We have to account for that\n",
    "main_df['Date'] = main_df['Date'] + ' ' + str(2023)\n",
    "main_df['Date'] = pd.to_datetime(main_df['Date'], format='%b %d %Y')\n",
    "try:\n",
    "    main_df.loc[main_df['Date'].dt.month == 1, 'Date'] = main_df['Date'] + pd.DateOffset(years=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = \"E:\\Data analytics projects\\Financial Tracker\\main_df.csv\"\n",
    "main_df.to_csv(output_csv_path, index=False, mode='a', header=not os.path.exists(output_csv_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinTrackerenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
